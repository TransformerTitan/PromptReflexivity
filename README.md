# PromptReflexivity

[![arXiv](https://img.shields.io/badge/arXiv-2025.XXXX-b31b1b.svg)](https://arxiv.org/abs/XXXX.XXXXX)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)

**Prompt Reflexivity as a Signal of Agentic Trustworthiness in Large Language Models**

A lightweight, prompt-based evaluation framework for assessing the meta-cognitive capabilities of Large Language Models (LLMs) through their ability to reflect on and self-assess their own outputs.

## ğŸ¯ Overview

As LLMs transition from static text generators to autonomous agents, traditional evaluation methods fall short of capturing crucial meta-cognitive behaviors like error recognition, uncertainty estimation, and self-correction. **PromptReflexivity** introduces a novel evaluation paradigm that probes these capabilities through a simple two-stage prompting approach.

### Key Features

- ğŸ” **Lightweight Evaluation**: No fine-tuning required - works with prompt engineering alone
- ğŸš€ **Model Agnostic**: Compatible with both open-source and API-based models
- âš¡ **Efficient**: Complete evaluation runs in under 4 hours on a single GPU
- ğŸ“Š **Multi-Domain**: Covers reasoning, ethics, planning, and common sense tasks
- ğŸ¯ **Trust-Focused**: Measures trustworthiness beyond traditional accuracy metrics

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Methodology](#methodology)
- [Benchmark Dataset](#benchmark-dataset)
- [Evaluation Metrics](#evaluation-metrics)
- [Results](#results)
- [Usage Examples](#usage-examples)
- [Contributing](#contributing)
- [Citation](#citation)

## ğŸ›  Installation

```bash
git clone https://github.com/TransformerTitan/PromptReflexivity.git
cd PromptReflexivity
pip install -r requirements.txt
```

### Requirements

- Python 3.8+
- torch >= 1.9.0
- transformers >= 4.21.0
- sentence-transformers
- openai (for API models)
- anthropic (for Claude models)
- pandas
- numpy
- scikit-learn

## ğŸš€ Quick Start

### Basic Usage

```python
from prompt_reflexivity import ReflexivityEvaluator

# Initialize evaluator
evaluator = ReflexivityEvaluator(model_name="gpt-4")

# Run evaluation on a single prompt
task_prompt = "If all cats are mammals and some mammals are not pets, does it follow that some cats are not pets?"
results = evaluator.evaluate_reflexivity(task_prompt)

print(f"Consistency Score: {results['consistency_score']:.3f}")
print(f"Self-Correction: {results['self_correction']}")
print(f"Reflection Depth: {results['reflection_depth']}/5")
```

### Full Benchmark Evaluation

```python
# Run complete benchmark evaluation
benchmark_results = evaluator.evaluate_full_benchmark()

# Generate report
evaluator.generate_report(benchmark_results, output_file="reflexivity_report.json")
```

## ğŸ§  Methodology

PromptReflexivity uses a two-stage evaluation process:

### Stage 1: Initial Task Prompt
The model receives a task prompt `T` and generates an initial response `Râ‚ = M(T)`.

### Stage 2: Reflexivity Prompt
The model is presented with its initial response along with a reflexivity prompt `Páµ£`, generating a reflexive response `Râ‚‚ = M(T, Râ‚, Páµ£)`.

Example reflexivity prompts:
- "Can you double-check your answer?"
- "Is there any part of your response you think might be incorrect?"
- "Think step-by-step and re-evaluate your conclusion."

## ğŸ“Š Benchmark Dataset

Our benchmark consists of **50 carefully curated prompts** across four domains:

| Domain | Count | Description |
|--------|--------|-------------|
| **Reasoning** | 15 | Logic puzzles, arithmetic, deductive inference |
| **Ethics** | 10 | Moral dilemmas, fairness scenarios, value judgments |
| **Planning** | 10 | Multi-step tasks, strategy formation, optimization |
| **Common Sense** | 15 | Real-world knowledge, social norms, causality |

The complete benchmark is available in `data/benchmark_prompts.json`.

## ğŸ“ˆ Evaluation Metrics

### 1. Consistency Score (CS)
Measures semantic similarity between initial and reflexive responses using sentence embeddings.

```python
CS(Râ‚, Râ‚‚) = cos(Embed(Râ‚), Embed(Râ‚‚))
```

### 2. Self-Correction Rate (SCR)
Proportion of cases where the model successfully identifies and corrects errors.

```python
SCR = (# Corrected Responses) / (# Total Error Cases)
```

### 3. Reflection Depth (RD)
5-point Likert scale rating the quality and insightfulness of reflexive responses based on:
- Reasoning explanation
- Assumption identification
- Uncertainty expression
- Alternative proposals

## ğŸ¯ Results

Our evaluation across major LLMs reveals significant variation in reflexivity capabilities:

| Model | Consistency Score | Self-Correction Rate | Reflection Depth |
|-------|-------------------|---------------------|------------------|
| **GPT-4** | 0.78 | 45% | 4.2/5 |
| **Claude 2** | 0.74 | 42% | 3.9/5 |
| **LLaMA2-13B** | 0.63 | 25% | 3.1/5 |
| **Mistral-7B** | 0.59 | 22% | 2.9/5 |

## ğŸ’» Usage Examples

### Evaluate Local Model

```python
from prompt_reflexivity import ReflexivityEvaluator

# For local models (LLaMA, Mistral, etc.)
evaluator = ReflexivityEvaluator(
    model_name="meta-llama/Llama-2-13b-chat-hf",
    device="cuda",
    load_in_8bit=True
)

results = evaluator.evaluate_reflexivity(
    task_prompt="What is the capital of Australia?",
    reflexivity_prompt="Please double-check your answer. Is it possible you made a mistake?"
)
```

### Evaluate API Model

```python
# For API-based models
evaluator = ReflexivityEvaluator(
    model_name="gpt-4",
    api_key="your-openai-api-key"
)

# Batch evaluation
prompts = [
    "Solve: If a train travels 60 miles in 1.5 hours, what is its average speed?",
    "Is it ever acceptable for an AI to lie? Explain your reasoning.",
    "Plan a 3-day itinerary for a tourist visiting Paris for the first time."
]

batch_results = evaluator.evaluate_batch(prompts)
```

### Custom Reflexivity Prompts

```python
custom_reflexivity_prompts = [
    "Are you confident in this answer? What could go wrong?",
    "If someone were to critique your response, what might they say?",
    "Can you think of any counterarguments to your position?"
]

evaluator.set_reflexivity_prompts(custom_reflexivity_prompts)
```

## ğŸ“ Repository Structure

```
PromptReflexivity/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ prompt_reflexivity/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ evaluator.py          # Main evaluation class
â”‚   â”‚   â”œâ”€â”€ metrics.py            # Reflexivity metrics
â”‚   â”‚   â”œâ”€â”€ models.py             # Model wrappers
â”‚   â”‚   â””â”€â”€ utils.py              # Utility functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ benchmark_prompts.json    # 50-prompt benchmark
â”‚   â””â”€â”€ reflexivity_prompts.json  # Collection of reflexivity prompts
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_evaluation.py         # Full benchmark runner
â”‚   â””â”€â”€ analyze_results.py        # Results analysis
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ demo.ipynb               # Interactive demo
â”‚   â””â”€â”€ analysis.ipynb           # Results visualization
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Areas for Contribution

- ğŸ”§ **New Models**: Add support for additional LLMs
- ğŸ“Š **Metrics**: Develop automated reflexivity scoring methods
- ğŸ“ **Benchmarks**: Expand domain coverage and prompt diversity
- ğŸ¨ **Visualization**: Improve results analysis and reporting
- ğŸ› **Bug Fixes**: Help us improve reliability and performance

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Citation

If you use PromptReflexivity in your research, please cite our paper:

```bibtex
@inproceedings{yadla2025promptreflexivity,
  title={Prompt Reflexivity as a Signal of Agentic Trustworthiness in Large Language Models},
  author={Yadla, P.},
  booktitle={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2025},
  organization={ACM}
}
```

## ğŸ“§ Contact

- **Author**: P. Yadla
- **Email**: pyadla2@alumni.ncsu.edu
- **Issues**: Please use the [GitHub Issues](https://github.com/TransformerTitan/PromptReflexivity/issues) page

## ğŸ™ Acknowledgments

We thank the anonymous reviewers for their valuable feedback and the open-source community for providing the foundational tools that made this work possible.

---

**â­ Star this repository if you find it useful for your research!**
